{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatGLM LoRA Finetune\n",
    "\n",
    "此Jupyter内核运行在服务器上，可以直接加载模型到本机的cuda上，使用的显卡为RTX A6000。\n",
    "使用LoRA的方法对ChatGLM进行监督微调。\n",
    "参考项目：\n",
    "> https://github.com/mymusise/ChatGLM-Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据加载\n",
    "读取数据并tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "\n",
    "\n",
    "model_name = \"THUDM/chatglm-6b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "config = transformers.AutoConfig.from_pretrained(model_name, trust_remote_code=True, device_map='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def preprocess_data(tokenizer, config, title, context):\n",
    "        prompt = f\"Instruction: 请根据新闻内容帮我写一个新闻短标题：\\n{context}\\nAnswer: \"\n",
    "        target = title\n",
    "        prompt_ids = tokenizer.encode(prompt)\n",
    "        target_ids = tokenizer.encode(target)\n",
    "        input_ids = prompt_ids + target_ids + [config.eos_token_id]\n",
    "        return {\"input_ids\": input_ids, \"seq_len\": len(prompt_ids)}\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def read_data(path, tokenizer, config,):\n",
    "        df_trian_data = pd.read_csv(path)\n",
    "        for row in df_trian_data.iterrows():\n",
    "                yield preprocess_data(tokenizer, config, title = row[1]['title'], context = row[1]['context'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/tmh/.cache/huggingface/datasets/generator/default-5c2623fc586f8070/0.0.0)\n",
      "                                                                                               \r"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "# 读取数据\n",
    "dataset = datasets.Dataset.from_generator(\n",
    "    lambda: read_data(\"data/train_data.csv\",tokenizer, config)\n",
    ")\n",
    "dataset.save_to_disk(\"train_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset generator (/home/tmh/.cache/huggingface/datasets/generator/default-b10cf1583203cada/0.0.0)\n",
      "                                                                                            \r"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Dataset.from_generator(\n",
    "    lambda: read_data(\"data/test_data.csv\",tokenizer, config)\n",
    ")\n",
    "dataset.save_to_disk(\"test_data\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /home/tmh/anaconda3/envs/glm/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 113\n",
      "CUDA SETUP: Loading binary /home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda113.so...\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "from transformers import Trainer\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "import datasets\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 加载模型与数据\n",
    "def load_model(peft_config = None):\n",
    "    # 设置LoRA参数\n",
    "    if peft_config is None : peft_config = default_peft_config\n",
    "    model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "    model.gradient_checkpointing_enable()\n",
    "    model.enable_input_require_grads()\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True\n",
    "    model.config.use_cache = (False)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将文本转换成神经网络模型所需的数据格式\n",
    "def data_collator(features: list) -> dict:\n",
    "    len_ids = [len(feature[\"input_ids\"]) for feature in features]\n",
    "    longest = max(len_ids)\n",
    "    input_ids = []\n",
    "    labels_list = []\n",
    "    for ids_l, feature in sorted(zip(len_ids, features), key=lambda x: -x[0]):\n",
    "        ids = feature[\"input_ids\"]\n",
    "        seq_len = feature[\"seq_len\"]\n",
    "        labels = (\n",
    "            [-100] * (seq_len - 1) + ids[(seq_len - 1) :] + [-100] * (longest - ids_l)\n",
    "        )\n",
    "        ids = ids + [tokenizer.pad_token_id] * (longest - ids_l)\n",
    "        _ids = torch.LongTensor(ids)\n",
    "        labels_list.append(torch.LongTensor(labels))\n",
    "        input_ids.append(_ids)\n",
    "    input_ids = torch.stack(input_ids)\n",
    "    labels = torch.stack(labels_list)\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"labels\": labels,\n",
    "    }\n",
    "\n",
    "\n",
    "class ModifiedTrainer(Trainer):\n",
    "    # def compute_loss(self, model, inputs, return_outputs=False):\n",
    "    #     return model(\n",
    "    #         input_ids=inputs[\"input_ids\"],\n",
    "    #         labels=inputs[\"labels\"],\n",
    "    #     ).loss\n",
    "\n",
    "    def save_model(self, output_dir=None, _internal_call=False):\n",
    "        from transformers.trainer import TRAINING_ARGS_NAME\n",
    "\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))\n",
    "        saved_params = {\n",
    "            k: v.to(\"cpu\") for k, v in self.model.named_parameters() if v.requires_grad\n",
    "        }\n",
    "        torch.save(saved_params, os.path.join(output_dir, \"adapter_model.bin\"))\n",
    "\n",
    "\n",
    "def run_training(model, train_args, train_dataset, eval_dataset, test_mode = False):\n",
    "    if test_mode:\n",
    "        train_args = TrainingArguments(\n",
    "            output_dir='output_test',\n",
    "            per_device_train_batch_size=3,\n",
    "            gradient_accumulation_steps=1,\n",
    "            max_steps=20,\n",
    "            save_steps=10,\n",
    "            save_total_limit=2,\n",
    "            learning_rate=1e-4,\n",
    "            remove_unused_columns=False,\n",
    "            logging_steps=5,\n",
    "            fp16=True\n",
    "        )\n",
    "        eval_dataset = None\n",
    "\n",
    "    trainer = ModifiedTrainer(\n",
    "        model=model,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        args=train_args,\n",
    "        data_collator=data_collator,\n",
    "    )\n",
    "    # start train\n",
    "    trainer.train()\n",
    "    model.save_pretrained(train_args.output_dir)\n",
    "    del model,trainer\n",
    "    torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.21it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:35, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>6.735700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>6.737400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.418800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 先训练20个Step测试一下训练效果\n",
    "torch.cuda.empty_cache()\n",
    "model_test = load_model()\n",
    "run_training(model=model_test, train_args = None,train_dataset=dataset, eval_dataset=None, test_mode=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "# 加载模型\n",
    "def get_infer_model(peft_config,lora_path = 'output_test', test_mode = False):\n",
    "    \n",
    "    if test_mode : peft_config = default_peft_config\n",
    "    torch.set_default_tensor_type(torch.cuda.HalfTensor) # type: ignore\n",
    "    infer_model = AutoModel.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True).half().cuda()\n",
    "    infer_model = get_peft_model(infer_model, peft_config)\n",
    "    infer_model.load_state_dict(torch.load(lora_path+'/adapter_model.bin'), strict=False)\n",
    "    torch.set_default_tensor_type(torch.cuda.FloatTensor) # type: ignore\n",
    "    return infer_model\n",
    "\n",
    "def generate_finetuned_title(context, model, tokenizer):\n",
    "    with torch.no_grad():\n",
    "        prompt = f\"Instruction: 请根据新闻内容帮我写一个新闻短标题：\\n{context}\\nAnswer: \"\n",
    "        response = model.chat(tokenizer,\n",
    "                                   prompt,\n",
    "                                   history=[],\n",
    "                                   max_length=2048,\n",
    "                                   top_p= 0.7,\n",
    "                                   temperature= 0.95)[0] # type: ignore\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def format_generate_result(result):\n",
    "    # 通过测试集数据发现可能存在以下情况：“新闻标题是：\\n”、“新闻短标题是：”，或者直接列出多个标题，或者生成一长段话\n",
    "    # 删除句号及句号之后的内容,处理生成一长段的情况\n",
    "    result = result.split(\"。\")[0]\n",
    "    # 处理“新闻标题是：\\n”、“新闻短标题是：”的情况\n",
    "    if \"：\\n\" in result and \"标题\" in result:\n",
    "        result = result.split(\"：\\n\")[1]\n",
    "    elif \"：\" in result and \"标题\" in result:\n",
    "        result = result.split(\"：\")[1]\n",
    "    # 处理多个标题的情况\n",
    "    result = result.split(\"\\n\")[0]    \n",
    "    return result\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "def run_infer(name,generate_method,infer_model,save_path = \"\",read_path = 'data/test_data.csv',saving_step=20,test_mode=False,append=False):\n",
    "    df = pd.read_csv(read_path)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"THUDM/chatglm-6b\", trust_remote_code=True)\n",
    "    if not append or name not in df.columns:\n",
    "        df[name] = pd.Series()\n",
    "    if test_mode:\n",
    "        max_iter = 5\n",
    "        saving_step = 1\n",
    "    else: \n",
    "        max_iter = len(df)\n",
    "    for i in tqdm(range(max_iter)):\n",
    "        if df[name].isna()[i] or df[name].iloc[i] == \"ChatGLM调用失败\" :\n",
    "            title,context = df.iloc[i,0],df.iloc[i,1]\n",
    "            # print(title,context)\n",
    "            result = generate_method(context,infer_model,tokenizer)\n",
    "            result = format_generate_result(result)\n",
    "            if test_mode :\n",
    "                print(f'{title}-----{result}')\n",
    "            else:\n",
    "                df[name].iloc[i] = result\n",
    "                if save_path != \"\":\n",
    "                    if i%saving_step==0 or i==max_iter-1:\n",
    "                        df.to_csv(save_path,index=False)\n",
    "    del infer_model\n",
    "    torch.cuda.empty_cache()\n",
    "    return df\n",
    "\n",
    "def run_infer_test(name,generate_method,infer_model):\n",
    "    run_infer(name,generate_method,infer_model=infer_model,test_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.16it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/tmp/ipykernel_1333/2187787770.py:20: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df[name] = pd.Series()\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]The dtype of attention mask (torch.int64) is not bool\n",
      " 20%|██        | 1/5 [00:02<00:10,  2.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "刘烨儿子取名诺一：生个女儿就叫“千金”-----刘烨给儿子取好名字，Angelababy成为《建党伟业》小凤仙\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [00:04<00:05,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "乡村乐组合Sugarland遭前队友索赔1400万美元-----Sugarland组合诉讼前队友 双方庭外和解\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [00:13<00:10,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "彩民周刊10139期双色球：一区2路号已亟待回补-----下期遗漏值综述：07、10、13、19、33、09、25、15、16、22、27、28、25、02、12、05、06、08、11、14、20、29、31、23、26、30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [00:15<00:03,  3.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女子锤杀出轨丈夫割下其生殖器(图)-----女子锤杀丈夫并割下其生殖器 法院轻判凶手\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:16<00:00,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "北大考研记：打开双手世界就在你手中-----考研成功的背后：与北大的情缘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "infer_model = get_infer_model('',test_mode=True)\n",
    "run_infer_test(name='finetuned_title' ,generate_method = generate_finetuned_title,infer_model=infer_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_path = \"result/test_result.csv\"\n",
    "val_result_path = \"result/val_result.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建保存结果的csv\n",
    "def create_result_csv(path,raw_csv_path):\n",
    "    df_raw = pd.read_csv(raw_csv_path)\n",
    "    df_result = pd.DataFrame(columns=['title','context'])\n",
    "    df_result['title'] = df_raw['title']\n",
    "    df_result['context'] = df_raw['context']\n",
    "    df_result.to_csv(path,index=False)\n",
    "\n",
    "\n",
    "create_result_csv(\"result/test_result.csv\",\"data/test_data.csv\")\n",
    "create_result_csv(\"result/val_result.csv\",\"data/val_data.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = datasets.load_from_disk('train_data')\n",
    "test_dataset = datasets.load_from_disk('test_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.33it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1200' max='1200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1200/1200 34:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.879100</td>\n",
       "      <td>2.257464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.250800</td>\n",
       "      <td>2.168503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.268000</td>\n",
       "      <td>2.119374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.085400</td>\n",
       "      <td>2.088719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.182000</td>\n",
       "      <td>2.067272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>2.078900</td>\n",
       "      <td>2.059705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.197300</td>\n",
       "      <td>2.039689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>2.003100</td>\n",
       "      <td>2.028336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>2.050800</td>\n",
       "      <td>2.015060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.027400</td>\n",
       "      <td>2.005554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>2.029400</td>\n",
       "      <td>1.997581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>2.090200</td>\n",
       "      <td>1.997920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "peft_config_underfit = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "train_args = TrainingArguments(\n",
    "    output_dir='test001',\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size =2,\n",
    "    gradient_accumulation_steps=1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    "    learning_rate=1e-4,\n",
    "    remove_unused_columns=False,\n",
    "    logging_steps=100,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = load_model(peft_config=peft_config_underfit)\n",
    "run_training(model=model, train_args = train_args,train_dataset=train_dataset, eval_dataset=test_dataset)\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.26it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [900/900 2:31:13, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>7.174900</td>\n",
       "      <td>6.815646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>6.422200</td>\n",
       "      <td>5.667855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.755700</td>\n",
       "      <td>3.275124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.643500</td>\n",
       "      <td>2.327520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.302300</td>\n",
       "      <td>2.176720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.110800</td>\n",
       "      <td>2.108420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.158400</td>\n",
       "      <td>2.068985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.124700</td>\n",
       "      <td>2.055057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.044300</td>\n",
       "      <td>2.026248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.060000</td>\n",
       "      <td>2.011586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.894800</td>\n",
       "      <td>2.016000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.882500</td>\n",
       "      <td>1.991819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.884000</td>\n",
       "      <td>2.006140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.846700</td>\n",
       "      <td>2.000502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.909800</td>\n",
       "      <td>1.982672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>1.697500</td>\n",
       "      <td>1.982744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>510</td>\n",
       "      <td>1.630600</td>\n",
       "      <td>2.010240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>1.609600</td>\n",
       "      <td>1.982849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>570</td>\n",
       "      <td>1.660400</td>\n",
       "      <td>1.995916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.654000</td>\n",
       "      <td>1.967070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>1.303000</td>\n",
       "      <td>2.020578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>1.298200</td>\n",
       "      <td>2.033613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>690</td>\n",
       "      <td>1.426900</td>\n",
       "      <td>2.005858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>1.361500</td>\n",
       "      <td>2.016787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.412500</td>\n",
       "      <td>2.041153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.972700</td>\n",
       "      <td>2.116922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>810</td>\n",
       "      <td>1.065600</td>\n",
       "      <td>2.085922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>1.030800</td>\n",
       "      <td>2.080128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>870</td>\n",
       "      <td>1.130100</td>\n",
       "      <td>2.094260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.173800</td>\n",
       "      <td>2.060366</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"test005\",\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    logging_steps=30,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=6,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model = load_model(peft_config=peft_config)\n",
    "run_training(model=model, train_args = train_args,train_dataset=train_dataset, eval_dataset=test_dataset)\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='451' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 1:23:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>6.280700</td>\n",
       "      <td>5.988694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>6.017600</td>\n",
       "      <td>5.738894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>5.678100</td>\n",
       "      <td>5.256823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.945100</td>\n",
       "      <td>4.431996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>4.015900</td>\n",
       "      <td>3.367702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.165900</td>\n",
       "      <td>2.666406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>2.461400</td>\n",
       "      <td>2.378569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.310800</td>\n",
       "      <td>2.269515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>2.248500</td>\n",
       "      <td>2.205420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.280000</td>\n",
       "      <td>2.166121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>2.061800</td>\n",
       "      <td>2.119721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.119000</td>\n",
       "      <td>2.107955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>2.165800</td>\n",
       "      <td>2.093791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>2.121600</td>\n",
       "      <td>2.071630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>2.047600</td>\n",
       "      <td>2.079918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>2.184400</td>\n",
       "      <td>2.064114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>1.989100</td>\n",
       "      <td>2.059405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>2.086900</td>\n",
       "      <td>2.037241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>2.056300</td>\n",
       "      <td>2.025315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.051300</td>\n",
       "      <td>2.023193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>1.866600</td>\n",
       "      <td>2.010072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>1.905400</td>\n",
       "      <td>2.026243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>1.940900</td>\n",
       "      <td>2.014081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>1.819200</td>\n",
       "      <td>2.004101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>1.796700</td>\n",
       "      <td>2.016800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>1.956600</td>\n",
       "      <td>2.014719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>405</td>\n",
       "      <td>1.911300</td>\n",
       "      <td>1.992339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>1.769000</td>\n",
       "      <td>2.008301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>435</td>\n",
       "      <td>1.839400</td>\n",
       "      <td>2.003472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>\n",
       "    <div>\n",
       "      \n",
       "      <progress value='67' max='67' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [67/67 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "train_args = TrainingArguments(\n",
    "    output_dir=\"test004\",\n",
    "    per_device_train_batch_size=2, \n",
    "    per_device_eval_batch_size=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=15,\n",
    "    logging_steps=15,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.1,\n",
    "    warmup_steps=1_000,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    learning_rate=5e-4,\n",
    "    save_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False,\n",
    "    remove_unused_columns=False\n",
    ")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# model = load_model(peft_config=peft_config)\n",
    "run_training(model=model, train_args = train_args,train_dataset=train_dataset, eval_dataset=test_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del tokenizer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.15it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/tmp/ipykernel_2148/1206028844.py:21: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df[name] = pd.Series()\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]/tmp/ipykernel_2148/1206028844.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[name].iloc[i] = result\n",
      "100%|██████████| 200/200 [07:02<00:00,  2.11s/it]\n"
     ]
    }
   ],
   "source": [
    "peft_config_overfit = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=32,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "infer_model = get_infer_model(peft_config_overfit,lora_path='test005')\n",
    "run_infer(name='finetuned_overfit' ,generate_method = generate_finetuned_title,infer_model=infer_model,read_path='result/val_result.csv',save_path='result/val_result.csv')\n",
    "del infer_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config_justfit = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "infer_model = get_infer_model(peft_config_justfit,lora_path='test004')\n",
    "run_infer(name='finetuned_justfit' ,generate_method = generate_finetuned_title,infer_model=infer_model,read_path='result/val_result.csv',save_path='result/val_result.csv')\n",
    "del infer_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.34it/s]\n",
      "/home/tmh/anaconda3/envs/glm/lib/python3.10/site-packages/peft/tuners/lora.py:191: UserWarning: fan_in_fan_out is set to True but the target module is not a Conv1D. Setting fan_in_fan_out to False.\n",
      "  warnings.warn(\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "/tmp/ipykernel_2780/1206028844.py:21: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  df[name] = pd.Series()\n",
      "  0%|          | 0/200 [00:00<?, ?it/s]The dtype of attention mask (torch.int64) is not bool\n",
      "/tmp/ipykernel_2780/1206028844.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df[name].iloc[i] = result\n",
      "100%|██████████| 200/200 [07:48<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "peft_config_underfit = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "torch.cuda.empty_cache()\n",
    "infer_model = get_infer_model(peft_config_underfit,lora_path='test001')\n",
    "run_infer(name='finetuned_underfit' ,generate_method = generate_finetuned_title,infer_model=infer_model,read_path='result/val_result.csv',save_path='result/val_result.csv')\n",
    "del infer_model\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
